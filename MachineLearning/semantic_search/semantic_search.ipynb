{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Semantic Search Workshop\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this interactive notebook on semantic search! This notebook is designed to be a hands-on learning environment where you'll explore, implement, and optimize semantic search algorithms and techniques.\n",
    "\n",
    "### What is Semantic Search?\n",
    "\n",
    "Semantic search refers to search techniques that understand the *meaning* of a query, rather than just matching keywords. It allows for more intelligent retrieval of information by understanding the context, intent, and conceptual meaning behind search queries.\n",
    "\n",
    "### In this Workshop, You Will:\n",
    "\n",
    "1. Learn about embeddings and how they capture semantic meaning\n",
    "2. Explore document similarity using vector representations\n",
    "3. Build a simple but effective semantic search engine\n",
    "4. Optimize your search engine for better performance\n",
    "5. Evaluate and benchmark your implementation\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- Read through the explanation cells carefully\n",
    "- Complete the code in cells marked with TODOs\n",
    "- Check your work against the solution cells (but try to solve problems yourself first!)\n",
    "- Answer the checkpoint questions to reinforce your understanding\n",
    "- Feel free to experiment and modify the code to see how it affects results\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Required Libraries\n",
    "\n",
    "First, let's install and import the libraries we'll need for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers numpy scikit-learn pandas matplotlib faiss-cpu nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "from functools import lru_cache\n",
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a sample dataset for our exercises. We'll use a collection of news headlines and articles for our semantic search experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset of news headlines and short articles\n",
    "sample_articles = [\n",
    "    {\n",
    "        \"title\": \"New AI Model Breaks Performance Records\",\n",
    "        \"content\": \"Researchers have developed a new artificial intelligence model that surpasses previous benchmarks on multiple tasks. The model demonstrates superior performance in natural language understanding and generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Global Climate Summit Reaches Historic Agreement\",\n",
    "        \"content\": \"World leaders at the climate summit have reached a historic agreement to reduce carbon emissions by 50% by 2030. The deal includes financial commitments to support developing nations in their transition to clean energy.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Tech Company Launches Revolutionary Smartphone\",\n",
    "        \"content\": \"A leading technology company has unveiled its latest smartphone with groundbreaking features. The device includes a foldable screen, week-long battery life, and advanced AI capabilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Scientists Discover Potential Cancer Treatment\",\n",
    "        \"content\": \"Medical researchers have identified a new compound that shows promise in treating aggressive forms of cancer. Early clinical trials indicate the treatment may be effective with minimal side effects.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Stock Market Reaches All-Time High\",\n",
    "        \"content\": \"The stock market closed at a record high yesterday, with technology and healthcare sectors leading the gains. Analysts attribute the surge to strong corporate earnings and positive economic indicators.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"New Study Links Exercise to Improved Brain Function\",\n",
    "        \"content\": \"A recent study has found that regular exercise is directly linked to enhanced cognitive performance and brain health. Participants who exercised regularly showed significant improvements in memory and problem-solving abilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Renewable Energy Surpasses Coal for the First Time\",\n",
    "        \"content\": \"For the first time in history, electricity generated from renewable sources has exceeded that from coal on a global scale. Solar and wind power installations have grown exponentially over the past decade.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Major Data Breach Exposes Millions of User Accounts\",\n",
    "        \"content\": \"A major online platform has reported a significant data breach affecting millions of users worldwide. The compromised data includes email addresses, passwords, and in some cases, payment information.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Archeologists Uncover Ancient City in Remote Jungle\",\n",
    "        \"content\": \"A team of archeologists has discovered the remains of a previously unknown ancient city deep in the jungle. The site includes elaborate temples, plazas, and residential areas dating back over 2,000 years.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"New Law Aims to Reduce Plastic Pollution\",\n",
    "        \"content\": \"Lawmakers have passed new legislation aimed at drastically reducing single-use plastic waste. The law will ban certain plastic products and impose taxes on others to encourage more sustainable alternatives.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"AI System Outperforms Human Doctors in Diagnosis\",\n",
    "        \"content\": \"A newly developed artificial intelligence system has demonstrated greater accuracy than human physicians in diagnosing several common diseases. The AI can analyze medical images and patient data to provide fast and accurate diagnoses.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Electric Vehicle Sales Double in Past Year\",\n",
    "        \"content\": \"Sales of electric vehicles have more than doubled compared to the previous year. Increased model availability, improved battery technology, and growing charging infrastructure have contributed to the surge in adoption.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"International Space Station Welcomes New Crew\",\n",
    "        \"content\": \"A new crew of astronauts has successfully docked with the International Space Station. The international team will conduct scientific experiments and maintenance activities during their six-month mission in orbit.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Education Reform Bill Passes with Bipartisan Support\",\n",
    "        \"content\": \"A comprehensive education reform bill has passed with support from both major political parties. The legislation includes increased funding for schools, teacher salary improvements, and new standards for curriculum development.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Global Pandemic Response Shows Signs of Success\",\n",
    "        \"content\": \"Coordinated global efforts to combat the recent pandemic are showing positive results with declining infection rates in multiple countries. Vaccination campaigns and public health measures have played crucial roles in this progress.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display the dataset\n",
    "articles_df = pd.DataFrame(sample_articles)\n",
    "print(f\"Sample dataset contains {len(articles_df)} articles\")\n",
    "articles_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Embeddings Exploration\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are dense vector representations of words, sentences, or documents that capture semantic meaning. Unlike simple one-hot encodings, embeddings place semantically similar items close together in vector space.\n",
    "\n",
    "Modern embedding techniques like BERT and its variants can capture nuanced contextual meanings, making them powerful tools for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sentence transformer model\n",
    "# We'll use a lightweight but effective model\n",
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create embeddings for some simple sentences to see how semantic meaning is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences with different semantic relationships\n",
    "sentences = [\n",
    "    \"I love programming in Python\",               # 0\n",
    "    \"Python is my favorite programming language\", # 1 - semantically similar to 0\n",
    "    \"I enjoy coding with Python\",                 # 2 - semantically similar to 0 and 1\n",
    "    \"Programming languages are essential tools\",  # 3 - related but less similar\n",
    "    \"Python snakes are fascinating reptiles\",     # 4 - different meaning despite \"Python\"\n",
    "    \"The weather is beautiful today\"              # 5 - completely different meaning\n",
    "]\n",
    "\n",
    "# TODO: Generate embeddings for these sentences using the model\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "print(f\"Sample of first embedding vector: {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the similarity between these sentence embeddings using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the cosine similarity between all pairs of sentences\n",
    "# and visualize the result as a heatmap\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        similarity_matrix[i, j] = util.pytorch_cos_sim(embeddings[i], embeddings[j]).item()\n",
    "\n",
    "# Create a heatmap visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='YlOrRd')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "\n",
    "# Add labels and annotations\n",
    "sentence_labels = [f\"Sentence {i+1}\" for i in range(len(sentences))]\n",
    "plt.xticks(np.arange(len(sentences)), sentence_labels, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(sentences)), sentence_labels)\n",
    "\n",
    "# Add text annotations in the cells\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        plt.text(j, i, f\"{similarity_matrix[i, j]:.2f}\",\n",
    "                 ha=\"center\", va=\"center\", color=\"black\" if similarity_matrix[i, j] < 0.8 else \"white\")\n",
    "\n",
    "plt.title('Semantic Similarity Between Sentences')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Questions:\n",
    "\n",
    "1. What do you observe in the similarity matrix? Which sentences are most similar to each other and why?\n",
    "2. Does the presence of the word 'Python' automatically make sentences semantically similar?\n",
    "3. How well does the model capture the meaning of the sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Similarity Comparison\n",
    "\n",
    "Now that we've explored basic similarities between sentences, let's apply these concepts to compare longer documents. We'll use our news article dataset for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's combine the title and content of each article to create full documents\n",
    "documents = []\n",
    "for article in sample_articles:\n",
    "    doc = f\"{article['title']}. {article['content']}\"\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(documents)} documents for analysis\")\n",
    "print(f\"Sample document: {documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding Strategies\n",
    "\n",
    "When dealing with longer documents, there are several embedding strategies to consider:\n",
    "\n",
    "1. **Full document embedding**: Encode the entire document at once (limited by token length)\n",
    "2. **Chunk-based embedding**: Break document into chunks, encode each chunk, then average/pool\n",
    "3. **Hierarchical embedding**: Combine sentence-level embeddings with document-level structure\n",
    "\n",
    "For this exercise, we'll start with the simplest approach - full document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate embeddings for the documents and calculate their similarity\n",
    "# Implement a function that takes two document indices and returns their similarity score\n",
    "# Your code here:\n",
    "\n",
    "def document_similarity(doc_idx1, doc_idx2):\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Also create a function to find the most similar document to a given document\n",
    "def find_most_similar(doc_idx, top_k=3):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate document embeddings\n",
    "doc_embeddings = model.encode(documents)\n",
    "print(f\"Generated embeddings for {len(doc_embeddings)} documents with dimension {doc_embeddings.shape[1]}\")\n",
    "\n",
    "def document_similarity(doc_idx1, doc_idx2):\n",
    "    \"\"\"Calculate the cosine similarity between two documents\"\"\"\n",
    "    emb1 = doc_embeddings[doc_idx1]\n",
    "    emb2 = doc_embeddings[doc_idx2]\n",
    "    similarity = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "    return similarity\n",
    "\n",
    "def find_most_similar(doc_idx, top_k=3):\n",
    "    \"\"\"Find the top-k most similar documents to the given document\"\"\"\n",
    "    query_embedding = doc_embeddings[doc_idx]\n",
    "    \n",
    "    # Calculate similarities with all documents\n",
    "    similarities = [util.pytorch_cos_sim(query_embedding, doc_emb).item() \n",
    "                 for doc_emb in doc_embeddings]\n",
    "    \n",
    "    # Get top k indices (excluding the query document itself)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    top_indices = [idx for idx in sorted_indices if idx != doc_idx][:top_k]\n",
    "    \n",
    "    return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Example usage\n",
    "print(\"Example document:\")\n",
    "example_idx = 0\n",
    "print(f\"Title: {sample_articles[example_idx]['title']}\")\n",
    "print(\"\\nMost similar documents:\")\n",
    "similar_docs = find_most_similar(example_idx)\n",
    "for idx, score in similar_docs:\n",
    "    print(f\"- {sample_articles[idx]['title']} (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Search Engine\n",
    "\n",
    "Now let's build a basic semantic search engine that can find relevant articles based on natural language queries.\n",
    "\n",
    "### Basic Search Implementation\n",
    "We'll start with a simple implementation that:\n",
    "1. Takes a query string\n",
    "2. Converts it to an embedding\n",
    "3. Finds the most similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSemanticSearch:\n",
    "    def __init__(self, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def index_documents(self, documents):\n",
    "        \"\"\"Index the documents by computing their embeddings\"\"\"\n",
    "        self.documents = documents\n",
    "        self.embeddings = self.model.encode(documents)\n",
    "        return self\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search for documents similar to the query\"\"\"\n",
    "        # Encode the query\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [util.pytorch_cos_sim(query_embedding, doc_emb).item() \n",
    "                     for doc_emb in self.embeddings]\n",
    "        \n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [(self.documents[idx], similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Initialize and index our documents\n",
    "search_engine = SimpleSemanticSearch()\n",
    "search_engine.index_documents(documents)\n",
    "\n",
    "# Try some searches\n",
    "print(\"Testing the search engine:\\n\")\n",
    "test_queries = [\n",
    "    \"latest developments in artificial intelligence\",\n",
    "    \"environmental protection and climate change\",\n",
    "    \"medical breakthroughs and healthcare\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = search_engine.search(query)\n",
    "    print(\"Top results:\")\n",
    "    for doc, score in results:\n",
    "        print(f\"- Score {score:.3f}: {doc[:100]}...\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization\n",
    "\n",
    "Let's optimize our search engine for better performance with larger document collections.\n",
    "Key improvements will include:\n",
    "- FAISS indexing for faster similarity search\n",
    "- Batch processing for document encoding\n",
    "- Result caching\n",
    "\n",
    "## End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
