  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all documents\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "def document_similarity(doc_idx1, doc_idx2):\n",
    "    \"\"\"Calculate cosine similarity between two documents\"\"\"\n",
    "    return util.pytorch_cos_sim(doc_embeddings[doc_idx1], doc_embeddings[doc_idx2]).item()\n",
    "\n",
    "def find_most_similar(doc_idx, top_k=3):\n",
    "    \"\"\"Find the top-k most similar documents to the given document\"\"\"\n",
    "    query_embedding = doc_embeddings[doc_idx]\n",
    "    \n",
    "    # Calculate similarities with all documents\n",
    "    similarities = [util.pytorch_cos_sim(query_embedding, doc_emb).item() \n",
    "                 for doc_emb in doc_embeddings]\n",
    "    \n",
    "    # Get top k indices (excluding the query document itself)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    top_indices = [idx for idx in sorted_indices if idx != doc_idx][:top_k]\n",
    "    \n",
    "    return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Example usage\n",
    "print(\"Example document:\")\n",
    "example_idx = 0\n",
    "print(f\"Title: {sample_articles[example_idx]['title']}\")\n",
    "print(\"\\nMost similar documents:\")\n",
    "similar_docs = find_most_similar(example_idx)\n",
    "for idx, score in similar_docs:\n",
    "    print(f\"- {sample_articles[idx]['title']} (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Search Engine\n",
    "\n",
    "Now let's build a basic semantic search engine that can find relevant articles based on natural language queries.\n",
    "\n",
    "### Basic Search Implementation\n",
    "We'll start with a simple implementation that:\n",
    "1. Takes a query string\n",
    "2. Converts it to an embedding\n",
    "3. Finds the most similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSemanticSearch:\n",
    "    def __init__(self, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def index_documents(self, documents):\n",
    "        \"\"\"Index the documents by computing their embeddings\"\"\"\n",
    "        self.documents = documents\n",
    "        self.embeddings = self.model.encode(documents)\n",
    "        return self\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search for documents similar to the query\"\"\"\n",
    "        # Encode the query\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [util.pytorch_cos_sim(query_embedding, doc_emb).item() \n",
    "                     for doc_emb in self.embeddings]\n",
    "        \n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [(self.documents[idx], similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Initialize and index our documents\n",
    "search_engine = SimpleSemanticSearch()\n",
    "search_engine.index_documents(documents)\n",
    "\n",
    "# Try some searches\n",
    "print(\"Testing the search engine:\\n\")\n",
    "test_queries = [\n",
    "    \"latest developments in artificial intelligence\",\n",
    "    \"environmental protection and climate change\",\n",
    "    \"medical breakthroughs and healthcare\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = search_engine.search(query)\n",
    "    print(\"Top results:\")\n",
    "    for doc, score in results:\n",
    "        print(f\"- Score {score:.3f}: {doc[:100]}...\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization\n",
    "\n",
    "Let's optimize our search engine for better performance with larger document collections.\n",
    "Key improvements will include:\n",
    "- FAISS indexing for faster similarity search\n",
    "- Batch processing for document encoding\n",
    "- Result caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSemanticSearch:\n",
    "    def __init__(self, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.faiss_index = None\n",
    "        self._cache = {}\n",
    "    \n",
    "    def index_documents(self, documents, batch_size=32):\n",
    "        \"\"\"Index documents with batching and FAISS\"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Batch process documents\n",
    "        embeddings = []\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Build FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "        self.faiss_index.hnsw.efConstruction = 40\n",
    "        self.faiss_index.add(embeddings)\n",
    "        return self\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search with caching and FAISS\"\"\"\n",
    "        query_embedding = self.model.encode([query]).astype('float32')\n",
    "        \n",
    "        # Search the FAISS index\n",
    "        self.faiss_index.hnsw.efSearch = 20\n",
    "        distances, indices = self.faiss_index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Return results\n",
    "        return [(self.documents[idx], 1 - dist) \n",
    "                for idx, dist in zip(indices[0], distances[0])]\n",
    "\n",
    "# Compare performance\n",
    "def benchmark_search_engines(queries, repeats=3):\n",
    "    engines = {\n",
    "        'Simple': SimpleSemanticSearch().index_documents(documents),\n",
    "        'Optimized': OptimizedSemanticSearch().index_documents(documents)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, engine in engines.items():\n",
    "        start_time = time.time()\n",
    "        for _ in range(repeats):\n",
    "            for query in queries:\n",
    "                _ = engine.search(query)\n",
    "        avg_time = (time.time() - start_time) / (len(queries) * repeats)\n",
    "        results[name] = avg_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_search_engines(test_queries)\n",
    "for engine, avg_time in benchmark_results.items():\n",
    "    print(f\"{engine} Search Engine - Average time per query: {avg_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Benchmarking\n",
    "\n",
    "To evaluate our search engine, we'll look at several metrics:\n",
    "- Relevance of results\n",
    "- Response time\n",
    "- Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
  "source": [
    "class SearchEvaluator:\n",
    "    def __init__(self, search_engine):\n",
    "        self.search_engine = search_engine\n",
    "        \n",
    "    def evaluate_query(self, query, relevant_docs, top_k=3):\n",
    "        \"\"\"Evaluate a single query's results\"\"\"\n",
    "        results = self.search_engine.search(query, top_k=top_k)\n",
    "        retrieved_docs = [doc for doc, _ in results]\n",
    "        \n",
    "        # Calculate precision\n",
    "        relevant_retrieved = set(retrieved_docs) & set(relevant_docs)\n",
    "        precision = len(relevant_retrieved) / len(retrieved_docs)\n",
    "        \n",
    "        # Calculate recall\n",
    "        recall = len(relevant_retrieved) / len(relevant_docs)\n",
    "        \n",
    "        # Calculate MRR (Mean Reciprocal Rank)\n",
    "        mrr = 0\n",
    "        for rank, (doc, _) in enumerate(results, 1):\n",
    "            if doc in relevant_docs:\n",
    "                mrr = 1 / rank\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'mrr': mrr\n",
    "        }\n",
    "    \n",
    "    def evaluate_all(self, test_cases):\n",
    "        \"\"\"Evaluate multiple test cases\"\"\"\n",
    "        metrics = []\n",
    "        for query, relevant_docs in test_cases:\n",
    "            result = self.evaluate_query(query, relevant_docs)\n",
    "            metrics.append(result)\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_metrics = {\n",
    "            metric: np.mean([m[metric] for m in metrics])\n",
    "            for metric in ['precision', 'recall', 'mrr']\n",
    "        }\n",
    "        return avg_metrics\n",
    "\n",
    "# Example evaluation\n",
    "test_cases = [\n",
    "    (\"artificial intelligence developments\", \n",
    "     [doc for doc in documents if \"AI\" in doc or \"artificial intelligence\" in doc.lower()]),\n",
    "    (\"climate change and environment\", \n",
    "     [doc for doc in documents if \"climate\" in doc.lower() or \"environment\" in doc.lower()])\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = SearchEvaluator(OptimizedSemanticSearch().index_documents(documents))\n",
    "evaluation_results = evaluator.evaluate_all(test_cases)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric.upper()}: {value:.3f}\")"
