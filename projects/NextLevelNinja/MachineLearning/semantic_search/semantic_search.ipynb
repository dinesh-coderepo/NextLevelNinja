    "# Solution\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        similarity_matrix[i, j] = util.pytorch_cos_sim(embeddings[i], embeddings[j]).item()\n",
    "\n",
    "# Create a heatmap visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='YlOrRd')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "\n",
    "# Add labels and annotations\n",
    "sentence_labels = [f\"Sentence {i+1}\" for i in range(len(sentences))]\n",
    "plt.xticks(np.arange(len(sentences)), sentence_labels, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(sentences)), sentence_labels)\n",
    "\n",
    "# Add text annotations in the cells\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        plt.text(j, i, f\"{similarity_matrix[i, j]:.2f}\",\n",
    "                 ha=\"center\", va=\"center\", color=\"black\" if similarity_matrix[i, j] < 0.8 else \"white\")\n",
    "\n",
    "plt.title('Semantic Similarity Between Sentences')\n",
    "plt.tight_layout()\n",
    "plt.show()"
    "# TODO: Generate embeddings for the documents and calculate their similarity\n",
    "# Implement a function that takes two document indices and returns their similarity score\n",
    "# Your code here:\n",
    "\n",
    "def document_similarity(doc_idx1, doc_idx2):\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Also create a function to find the most similar document to a given document\n",
    "def find_most_similar(doc_idx, top_k=3):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate document embeddings\n",
    "doc_embeddings = model.encode(documents)\n",
    "print(f\"Generated embeddings for {len(doc_embeddings)} documents with dimension {doc_embeddings.shape[1]}\")\n",
    "\n",
    "def document_similarity(doc_idx1, doc_idx2):\n",
    "    \"\"\"Calculate the cosine similarity between two documents\"\"\"\n",
    "    emb1 = doc_embeddings[doc_idx1]\n",
    "    emb2 = doc_embeddings[doc_idx2]\n",
    "    similarity = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "    return similarity\n",
    "\n",
    "def find_most_similar(doc_idx, top_k=3):\n",
    "    \"\"\"Find the top-k most similar documents to the given document\"\"\"\n",
    \"    query_embedding = doc_embeddings[\r\n
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## Conclusion\n",
  "\n",
  "In this interactive workshop, you've learned how to:\n",
  "1. Work with embeddings and understand semantic similarity\n",
  "2. Build a basic semantic search engine\n",
  "3. Optimize search performance using advanced techniques\n",
  "4. Evaluate search quality using standard metrics\n",
  "\n",
  "Next steps could include:\n",
  "- Implementing more sophisticated ranking algorithms\n",
  "- Adding support for filtering and faceted search\n",
  "- Scaling to handle larger document collections\n",
  "- Fine-tuning the embedding model for your specific domain\n",
  "\n",
  "The techniques and concepts covered here form the foundation of modern semantic search systems. By understanding and implementing these components, you're well-equipped to build and optimize search functionality in your own applications."
 ]
}
